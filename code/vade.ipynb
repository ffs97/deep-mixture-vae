{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation Deep Embeddings\n",
    "\n",
    "Paper Link - https://arxiv.org/abs/1611.05148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "from includes.config import Config\n",
    "from includes.utils import get_data, Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import mixture\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.contrib.distributions\n",
    "xav_init = tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"spiral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config(data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = get_data(dataset)\n",
    "\n",
    "train_data = Dataset(train_data, batch_size=conf.batch_size)\n",
    "test_data = Dataset(test_data, batch_size=conf.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"spiral\":\n",
    "    plt.scatter(train_data.data[:, 0], train_data.data[:, 1], s=0.5)\n",
    "elif dataset == \"mnist\" or dataset == \"mmnist\":\n",
    "    images = train_data.data[:100]\n",
    "    if dataset == \"mmnist\":\n",
    "        images = images[:, :-1]\n",
    "    \n",
    "    images = images.reshape((100, 28, 28))\n",
    "    images = np.concatenate(images, axis=1)\n",
    "    images = np.array([images[:, x:x+280] for x in range(0, 2800, 280)])\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    images = np.concatenate(\n",
    "        [np.zeros((280, 10)), images, np.zeros((280, 10))], axis=1\n",
    "    )\n",
    "    images = np.concatenate(\n",
    "        [np.zeros((10, 300)), images, np.zeros((10, 300))], axis=0\n",
    "    )\n",
    "\n",
    "    plt.imshow(images, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.title(\"Test\")\n",
    "plt.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    global X\n",
    "    \n",
    "    h_encoders = [\n",
    "        tf.layers.dense(\n",
    "            X,\n",
    "            conf.encoder_hidden_size[0],\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=xav_init,\n",
    "            name=\"encoder_hidden_layer_0\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for index, size in enumerate(conf.encoder_hidden_size[1:]):\n",
    "        h_encoders.append(\n",
    "            tf.layers.dense(\n",
    "                h_encoders[index],\n",
    "                size,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=xav_init,\n",
    "                name=\"encoder_hidden_layer_\" + str(index + 1)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    encoder_mean = tf.layers.dense(\n",
    "        h_encoders[-1],\n",
    "        conf.latent_dim,\n",
    "        kernel_initializer=xav_init,\n",
    "        name=\"encoder_mean\"\n",
    "    )\n",
    "    encoder_log_var = tf.layers.dense(\n",
    "        h_encoders[-1],\n",
    "        conf.latent_dim,\n",
    "        kernel_initializer=xav_init,\n",
    "        name=\"encoder_log_variance\"\n",
    "    )\n",
    "    \n",
    "    return encoder_mean, encoder_log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder():\n",
    "    global Z\n",
    "    \n",
    "    h_decoders = [\n",
    "        tf.layers.dense(\n",
    "            Z,\n",
    "            conf.decoder_hidden_size[0],\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=xav_init,\n",
    "            name=\"decoder_hidden_layer_0\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for index, size in enumerate(conf.decoder_hidden_size[1:]):\n",
    "        h_decoders.append(\n",
    "            tf.layers.dense(\n",
    "                h_decoders[index],\n",
    "                size,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=xav_init,\n",
    "                name=\"decoder_hidden_layer_\" + str(index + 1)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    out_X = tf.layers.dense(\n",
    "        h_decoders[-1],\n",
    "        conf.input_dim,\n",
    "        kernel_initializer=xav_init,\n",
    "        name=\"decoder_X\"\n",
    "    )\n",
    "    \n",
    "    return out_X, tf.nn.sigmoid(out_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Z using the reparametrization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z():\n",
    "    global epsilon\n",
    "    global encoder_mean, encoder_log_var\n",
    "    \n",
    "    return encoder_mean + tf.exp(encoder_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and Learning the GMM Priors (Pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prior():\n",
    "    prior_means = tf.Variable(\n",
    "        tf.random_normal((conf.n_clusters, conf.latent_dim), stddev=5.0),\n",
    "        dtype=tf.float32,\n",
    "        name=\"prior_means\"\n",
    "    )\n",
    "    prior_vars = tf.Variable(\n",
    "        tf.ones((conf.n_clusters, conf.latent_dim)),\n",
    "        dtype=tf.float32,\n",
    "        name=\"prior_vars\"\n",
    "    )\n",
    "    prior_weights = tf.Variable(\n",
    "        tf.ones((conf.n_clusters)) / conf.n_clusters,\n",
    "        dtype=tf.float32,\n",
    "        name=\"prior_weights\"\n",
    "    )\n",
    "    \n",
    "    return prior_means, prior_vars, prior_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gmm_priors(Z=None, train=True):\n",
    "    global init_gmm_model\n",
    "    \n",
    "    if train == True:\n",
    "        init_gmm_model.fit(Z)\n",
    "        \n",
    "    return init_gmm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Posterior of Cluster Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_c():\n",
    "    global Z\n",
    "    global prior_means, prior_vars, prior_weights\n",
    "    \n",
    "    def fn_cluster(_, k):\n",
    "        q = prior_weights[k] * ds.MultivariateNormalDiag(loc=prior_means[k], scale_diag=prior_vars[k]).prob(Z) + 1e-10\n",
    "        return tf.reshape(q, [conf.batch_size])\n",
    "\n",
    "    clusters = tf.Variable(tf.range(conf.n_clusters))\n",
    "    probs = tf.scan(fn_cluster, clusters, initializer=tf.ones([conf.batch_size]))\n",
    "    probs = tf.transpose(probs)\n",
    "    probs = probs / tf.reshape(tf.reduce_sum(probs, 1), (-1, 1))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(dataset=\"mnist\"):\n",
    "    global cluster_weights\n",
    "    global X, decoded_exp_X_mean\n",
    "    global encoder_mean, encoder_log_var\n",
    "    global prior_means, prior_vars, prior_weights\n",
    "    \n",
    "    J = 0.0\n",
    "    if dataset == \"mnist\":\n",
    "        J += conf.regularizer * tf.reduce_sum(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=X, logits=decoded_exp_X_mean\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    elif dataset == \"spiral\":\n",
    "        J += conf.regularizer * tf.reduce_sum(\n",
    "            tf.square(decoded_exp_X_mean - X),\n",
    "            axis=1\n",
    "        )\n",
    "    J -= tf.reduce_sum(cluster_weights * tf.log(prior_weights), axis=1)\n",
    "    J += tf.reduce_sum(cluster_weights * tf.log(cluster_weights), axis=1)\n",
    "    J -= 0.5 * tf.reduce_sum(1 + encoder_log_var, axis=1)\n",
    "\n",
    "    def fn_cluster(previous_output, current_input):\n",
    "        k = current_input\n",
    "        l = previous_output + 0.5 * cluster_weights[:, k] * tf.reduce_sum(\n",
    "            tf.log(prior_vars[k]) + \n",
    "            (\n",
    "                tf.exp(encoder_log_var) + \n",
    "                tf.square(encoder_mean - prior_means[k])\n",
    "            ) / prior_vars[k], axis=1\n",
    "        )\n",
    "        return l\n",
    "\n",
    "    clusters = tf.Variable(tf.range(conf.n_clusters))\n",
    "    y = tf.scan(fn_cluster, clusters, initializer=tf.zeros(conf.batch_size))\n",
    "    \n",
    "    J += y[-1, :]\n",
    "    \n",
    "    return tf.reduce_mean(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the TensorFlow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, conf.input_dim])\n",
    "epsilon = tf.placeholder(tf.float32, [None, conf.latent_dim])\n",
    "\n",
    "prior_means, prior_vars, prior_weights = init_prior()\n",
    "\n",
    "encoder_mean, encoder_log_var = encoder()\n",
    "\n",
    "Z = sample_Z()\n",
    "\n",
    "decoded_exp_X_mean, decoded_X_mean = decoder()\n",
    "\n",
    "cluster_weights = q_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = vae_loss(dataset)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    learning_rate=conf.learning_rate,\n",
    "    global_step=0,\n",
    "    decay_steps=train_data.epoch_len * conf.decay_steps,\n",
    "    decay_rate=conf.decay_rate\n",
    ")\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(\n",
    "    learning_rate=learning_rate, \n",
    "    epsilon=conf.epsilon\n",
    ").minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Pretraining Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"mnist\":\n",
    "    pretrain_loss = tf.reduce_mean(\n",
    "        tf.reduce_sum(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=X, \n",
    "                logits=decoded_exp_X_mean\n",
    "            ), axis=1\n",
    "        ) + 0.5 * tf.reduce_sum(\n",
    "            tf.exp(encoder_log_var) + encoder_mean ** 2 - 1. - encoder_log_var,\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "elif dataset == \"spiral\":\n",
    "    pretrain_loss = tf.reduce_mean(\n",
    "        tf.reduce_sum(\n",
    "            tf.square(\n",
    "                decoded_exp_X_mean - X\n",
    "            ), axis=1\n",
    "        ) + 0.5 * tf.reduce_sum(\n",
    "            tf.exp(encoder_log_var) + encoder_mean ** 2 - 1. - encoder_log_var,\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    learning_rate=conf.learning_rate,\n",
    "    global_step=0,\n",
    "    decay_steps=train_data.epoch_len * conf.decay_steps,\n",
    "    decay_rate=conf.decay_rate\n",
    ")\n",
    "\n",
    "pretrain_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(pretrain_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Defining functions for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def regeneration_plot(epoch, data=\"mnist\"):\n",
    "    if not os.path.exists(\"plots/regenerated/\" + dataset):\n",
    "        os.makedirs(\"plots/regenerated/\" + dataset)\n",
    "\n",
    "    gs = grid.GridSpec(1, 2) \n",
    "\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "        \n",
    "    if data == \"mnist\":\n",
    "        decoded_image = sess.run(\n",
    "            decoded_X_mean,\n",
    "            feed_dict={\n",
    "                X: test_data.data[:100],\n",
    "                epsilon: np.random.randn(100, conf.latent_dim)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        decoded_image = np.array(decoded_image).reshape((100, 784))\n",
    "        figure = np.zeros((280, 280))\n",
    "\n",
    "        for i in range(0, 10):\n",
    "            for j in range(0, 10):\n",
    "                figure[i * 28 : (i + 1) * 28, j * 28 : (j + 1) * 28] = (\n",
    "                    decoded_image[10 * i + j].reshape((28, 28)) * 255\n",
    "                )\n",
    "\n",
    "        ax2.imshow(figure, cmap=\"Greys_r\")\n",
    "\n",
    "        decoded_image = np.array(test_data.data[:100])\n",
    "        figure = np.zeros((280, 280))\n",
    "\n",
    "        for i in range(0, 10):\n",
    "            for j in range(0, 10):\n",
    "                figure[i * 28 : (i + 1) * 28, j * 28 : (j + 1) * 28] = (\n",
    "                    decoded_image[10 * i + j].reshape((28, 28)) * 255\n",
    "                )\n",
    "\n",
    "        ax1.imshow(figure, cmap=\"Greys_r\")\n",
    "        \n",
    "    elif data == \"spiral\":\n",
    "        decoded_X = sess.run(\n",
    "            decoded_exp_X_mean,\n",
    "            feed_dict={\n",
    "                X: test_data.data,\n",
    "                epsilon: np.random.randn(len(test_data.data), conf.latent_dim)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        ax1.scatter(test_data.data[:, 0], test_data.data[:, 1], s=0.5)\n",
    "        ax2.scatter(decoded_X[:, 0], decoded_X[:, 1], s=0.5)\n",
    "        \n",
    "    ax1.spines['left'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "    ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax2.get_xaxis().set_visible(False)\n",
    "    ax2.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(\"plots/regenerated/\" + dataset + \"/\" + str(epoch) + \".png\", transparent=True)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sample_plot(epoch, dataset=\"mnist\"):\n",
    "    if not os.path.exists(\"plots/sampled/\" + dataset):\n",
    "        os.makedirs(\"plots/sampled/\" + dataset)\n",
    "    \n",
    "    gs = grid.GridSpec(1, 2)\n",
    "\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "    \n",
    "    mus, sigmas = sess.run([prior_means, prior_vars], feed_dict={})\n",
    "    \n",
    "    sigmas = np.sqrt(sigmas)\n",
    "    \n",
    "    if dataset == \"mnist\":\n",
    "        sample_Z = list()\n",
    "        decoded_X = list()\n",
    "        for k in range(0, conf.n_clusters):\n",
    "            s_Z = mus[k] + sigmas[k] * np.random.randn(1000, conf.latent_dim)\n",
    "            sample_Z.append(s_Z)\n",
    "\n",
    "            decoded_X.append(sess.run(\n",
    "                decoded_X_mean,\n",
    "                feed_dict={\n",
    "                    Z: s_Z\n",
    "                }\n",
    "            ))\n",
    "\n",
    "        sample_Z = np.concatenate(sample_Z, axis=0)\n",
    "        if conf.latent_dim > 2:\n",
    "            sample_Z = TSNE(n_components=2).fit_transform(sample_Z)\n",
    "        \n",
    "        sample_Z = sample_Z.reshape((conf.n_clusters, sample_Z.shape[0] / conf.n_clusters, 2))\n",
    "\n",
    "        image = (\n",
    "            1 - np.concatenate(\n",
    "                np.concatenate(\n",
    "                    np.array(decoded_X)[:, :10].reshape((10, 10, 28, 28)),\n",
    "                    axis=1\n",
    "                ), \n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ax1.imshow(image, cmap=\"Greys\")\n",
    "\n",
    "        for k in range(0, conf.n_clusters):\n",
    "            ax2.scatter(sample_Z[k][:, 0], sample_Z[k][:, 1], s=0.5)\n",
    "        \n",
    "    elif dataset == \"spiral\":\n",
    "        eps = np.random.randn(conf.n_clusters, 1000, conf.latent_dim)\n",
    "        \n",
    "        sample_Z = np.concatenate([eps[k] * sigmas[k] + mus[k] for k in range(0, conf.n_clusters)])\n",
    "        \n",
    "        decoded_X = [\n",
    "            sess.run(\n",
    "                decoded_exp_X_mean,\n",
    "                feed_dict={\n",
    "                    Z: sample_Z[1000*k:1000*(k + 1)]\n",
    "                }\n",
    "            ) for k in range(0, conf.n_clusters)\n",
    "        ]\n",
    "        \n",
    "        if conf.latent_dim > 2:\n",
    "            sample_Z = TSNE(n_components=2).fit_transform(sample_Z)\n",
    "\n",
    "        for k in range(0, conf.n_clusters):\n",
    "            ax1.scatter(decoded_X[k][:, 0], decoded_X[k][:, 1], s=0.5)\n",
    "            ax2.scatter(sample_Z[1000*k:1000*(k+1), 0], sample_Z[1000*k:1000*(k+1), 1], s=0.5)\n",
    "\n",
    "    ax1.spines['left'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "    ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax2.get_xaxis().set_visible(False)\n",
    "    ax2.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(\"plots/sampled/\" + dataset + \"/\" + str(epoch) + \".png\", transparent=True)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining for VAE parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tqdm(range(conf.pretrain_vae_n_epochs), postfix={\"loss\": float(\"inf\")}) as bar:\n",
    "    for epoch in bar:\n",
    "        J = 0.0\n",
    "        for batch in train_data.get_batches():\n",
    "            out = sess.run(\n",
    "                [pretrain_loss, pretrain_step],\n",
    "                feed_dict={\n",
    "                    X: batch,\n",
    "                    epsilon: np.random.randn(conf.batch_size, conf.latent_dim)\n",
    "                }\n",
    "            )\n",
    "            J += out[0] / train_data.epoch_len\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            regeneration_plot(epoch, dataset)\n",
    "\n",
    "        bar.set_postfix({\"loss\": J})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining for GMM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = sess.run(Z, feed_dict={\n",
    "    X: train_data.data,\n",
    "    epsilon: np.random.randn(len(train_data.data), conf.latent_dim)\n",
    "})\n",
    "init_gmm_model = mixture.GaussianMixture(\n",
    "    n_components=conf.n_clusters,\n",
    "    covariance_type=\"diag\",\n",
    "    max_iter=conf.pretrain_gmm_n_iters,\n",
    "        n_init=conf.pretrain_gmm_n_inits,\n",
    "    weights_init=np.ones(conf.n_clusters) / conf.n_clusters,\n",
    ")\n",
    "\n",
    "init_gmm_means = tf.assign(prior_means, init_gmm_priors(Z=lv).means_)\n",
    "init_gmm_vars = tf.assign(prior_vars, init_gmm_priors(train=False).covariances_)\n",
    "init_gmm_weights = tf.assign(prior_weights, init_gmm_priors(train=False).weights_)\n",
    "\n",
    "_ = sess.run([init_gmm_means, init_gmm_vars, init_gmm_weights], feed_dict={})\n",
    "\n",
    "sample_plot(0, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the VaDE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tqdm(range(conf.n_epochs), postfix={\"loss\": float(\"inf\")}) as bar:\n",
    "    for epoch in bar:\n",
    "        J = 0.0\n",
    "        for batch in train_data.get_batches():\n",
    "            out = sess.run(\n",
    "                [loss, train_step],\n",
    "                feed_dict={\n",
    "                    X: batch,\n",
    "                    epsilon: np.random.randn(conf.batch_size, conf.latent_dim)\n",
    "                }\n",
    "            )\n",
    "            J += out[0] / train_data.epoch_len\n",
    "\n",
    "        bar.set_postfix({\"loss\": J})\n",
    "    \n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            sample_plot(epoch + 1, dataset)\n",
    "            regeneration_plot(epoch, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (Machine Learning)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}